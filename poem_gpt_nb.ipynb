{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MPOFO7McDn0r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 config,\n",
        "                 vocab_size):\n",
        "        \"\"\"\n",
        "            Embedding generates learnable representation of an input sequence which encodes\n",
        "            contextual, semantic meaning for each word.\n",
        "            Params:\n",
        "                d_model(int): specifies the embedding dimension for each token/word\n",
        "                vocab_size(int): number of embeddings that would be needed. # of unique words\n",
        "                max_seq_len(int): the maximum sequence length of an input sequence. Used for generation positional encoding\n",
        "                dropout(float): probability of dropout applied on the final embedding output\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                                  embedding_dim=config[\"d_model\"])\n",
        "        self.position_embedding_table = nn.Embedding(num_embeddings=config[\"context_length\"],\n",
        "                                                     embedding_dim=config[\"d_model\"])\n",
        "        self.dropout = nn.Dropout(p=config[\"dropout\"])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x => [B, S]\n",
        "        B, S = x.shape\n",
        "        token_emb = self.token_embedding_table(x) # [B, S, D]\n",
        "\n",
        "        pos_emb = self.position_embedding_table(torch.arange(S, device=device)).unsqueeze(0) # [1, S, D]\n",
        "        out = self.dropout(token_emb+pos_emb)\n",
        "        return self.dropout(out)\n",
        "\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 config) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = config[\"d_model\"]\n",
        "        self.head_dim = config[\"head_dim\"]\n",
        "\n",
        "        self.query = nn.Linear(self.d_model, self.head_dim)\n",
        "        self.key = nn.Linear(self.d_model, self.head_dim)\n",
        "        self.value = nn.Linear(self.d_model, self.head_dim)\n",
        "        self.dropout = nn.Dropout(p=config[\"dropout\"])\n",
        "\n",
        "    def forward(self,\n",
        "                query: torch.Tensor,\n",
        "                key: torch.Tensor,\n",
        "                value: torch.Tensor,\n",
        "                mask=None) -> torch.Tensor:\n",
        "\n",
        "        # query => [B, Q, D]\n",
        "        # key => [B, K, D]\n",
        "        # value => [B, K, D]\n",
        "\n",
        "        q = self.query(query) # B, Q, HEAD_DIM\n",
        "        k = self.key(key) # B, K, HEAD_DIM\n",
        "        v = self.value(value) # B, K, HEAD_DIM\n",
        "\n",
        "        weights = q @ k.transpose(1, 2) # B, Q, K\n",
        "        if mask is not None:\n",
        "            weights = weights.masked_fill(mask==0, value=float(\"-inf\"))\n",
        "        weights = F.softmax(weights/math.sqrt(self.head_dim), dim=-1)\n",
        "        out = weights @ v # [B, Q, K] x [B, K, HEAD_DIM] => [B, Q, HEAD_DIM]\n",
        "        return self.dropout(out)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 config) -> None:\n",
        "\n",
        "         super().__init__()\n",
        "         self.sa_heads = nn.ModuleList([AttentionHead(config) for _ in range(config[\"n_heads\"])])\n",
        "         self.proj = nn.Linear(config[\"d_model\"], config[\"d_model\"])\n",
        "         self.dropout = nn.Dropout(p=config[\"dropout\"])\n",
        "\n",
        "    def forward(self,\n",
        "                query: torch.Tensor,\n",
        "                key: torch.Tensor,\n",
        "                value: torch.Tensor,\n",
        "                mask=None) -> torch.Tensor:\n",
        "\n",
        "        out = torch.cat([h(query, key, value, mask) for h in self.sa_heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return self.dropout(out)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 config):\n",
        "\n",
        "        super().__init__()\n",
        "        d_model = config[\"d_model\"]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model*4, d_model),\n",
        "            nn.Dropout(p=config[\"dropout\"])\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTDecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(config)\n",
        "        self.ff = FeedForward(config)\n",
        "        self.ln_1 = nn.LayerNorm(normalized_shape=config[\"d_model\"])\n",
        "        self.ln_2 = nn.LayerNorm(normalized_shape=config[\"d_model\"])\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
        "\n",
        "        x = x + self.mha(self.ln_1(x), self.ln_1(x), self.ln_1(x), mask)\n",
        "        x = x + self.ff(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPTDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([GPTDecoderBlock(config) for _ in range(config[\"n_decoders\"])])\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "        return x\n",
        "\n",
        "class PoemGPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config, vocab_size) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        self.context_length = config[\"context_length\"]\n",
        "        self.embedding = Embedding(config, vocab_size)\n",
        "        self.gpt = GPTDecoder(config)\n",
        "        self.lm_head = nn.Linear(config[\"d_model\"], vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                targets: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        B, S = x.shape\n",
        "        # x -> [B, S], targets -> [B, S]\n",
        "        x = self.embedding(x) # B, S, D_MODEL\n",
        "        mask = create_causal_mask(S)\n",
        "\n",
        "        x = self.gpt(x, mask) # B, S, D_MODEL\n",
        "        logits = self.lm_head(x) # B, S, VOCAB_SIZE\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.view(B*S, -1)\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, x:torch.Tensor=None, max_new_tokens: int=500) -> torch.Tensor:\n",
        "\n",
        "        if x is None:\n",
        "            x = torch.zeros((1, 1), dtype=torch.long, device=device) # B, S\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            preds, _ = self(x[:, -self.context_length:])# B, S, VOCAB_SIZE\n",
        "            preds = preds[:, -1, :] # B, VOCAB_SIZE\n",
        "            probs = F.softmax(preds, dim=-1)\n",
        "            x_next = torch.multinomial(input=probs, num_samples=1) # B, 1\n",
        "            x = torch.cat((x, x_next), dim=1) # B, S+1\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_causal_mask(sz):\n",
        "    mask = torch.ones((sz, sz), device=device)\n",
        "    mask = torch.tril(mask)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLZ2hS7cD7hh",
        "outputId": "67a9369d-d6ec-4c4f-c097-742416c6b6a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0 train_loss: 4.687802615165711 val_loss: 4.681776196956634\n",
            "iter 500 train_loss: 2.1590825736522676 val_loss: 2.215250650644302\n",
            "iter 1000 train_loss: 1.8130074471235276 val_loss: 1.9667431920766831\n",
            "iter 1500 train_loss: 1.6225154322385789 val_loss: 1.8205379575490952\n",
            "iter 2000 train_loss: 1.4965563523769378 val_loss: 1.7162453413009644\n",
            "iter 2500 train_loss: 1.4177560430765153 val_loss: 1.668158510327339\n",
            "iter 3000 train_loss: 1.3559211957454682 val_loss: 1.6206841945648194\n",
            "iter 3500 train_loss: 1.335439379811287 val_loss: 1.6167381221055985\n",
            "iter 4000 train_loss: 1.2825062787532806 val_loss: 1.5865983229875564\n",
            "iter 4500 train_loss: 1.2475514125823974 val_loss: 1.5702516168355942\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import Tuple, Dict\n",
        "import json\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "with open(\"./poem_gpt_config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "with open(\"./input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(data))\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "def get_random_batch(split: str=\"train\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "    data = train_data if split==\"train\" else val_data\n",
        "\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    block_size = config[\"block_size\"]\n",
        "\n",
        "    idxs = torch.randint(0, len(data)-block_size, size=(batch_size,))\n",
        "    x_batch = torch.stack([data[i:i+block_size] for i in idxs])\n",
        "    y_batch = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
        "\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    return x_batch, y_batch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model() -> Dict[str, float]:\n",
        "    losses = {}\n",
        "    poem_gpt.eval()\n",
        "\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        data = train_data if split==\"train\" else val_data\n",
        "        loss = 0\n",
        "        for iter in range(config[\"eval_iters\"]):\n",
        "            x_batch, y_batch = get_random_batch(split)\n",
        "            _, l_ = poem_gpt(x_batch, y_batch)\n",
        "            loss += l_.item()\n",
        "\n",
        "        losses[split] = loss/config[\"eval_iters\"]\n",
        "\n",
        "    poem_gpt.train()\n",
        "    return losses\n",
        "\n",
        "\n",
        "def train_poem_gpt():\n",
        "\n",
        "    for iter in range(config[\"train_iters\"]):\n",
        "\n",
        "        if iter%config[\"eval_interval\"]==0:\n",
        "            losses = eval_model()\n",
        "            print(f\"iter {iter} train_loss: {losses['train']} val_loss: {losses['val']}\")\n",
        "\n",
        "        x_batch, y_batch = get_random_batch()\n",
        "        _, loss = poem_gpt(x_batch, y_batch)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "poem_gpt = PoemGPT(config, vocab_size)\n",
        "poem_gpt = poem_gpt.to(device)\n",
        "optimizer = torch.optim.AdamW(params=poem_gpt.parameters(),\n",
        "                              lr=config[\"learning_rate\"])\n",
        "\n",
        "train_poem_gpt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfHty3qFG9cV",
        "outputId": "7269b7f9-0183-4126-f799-d8e0f2bd1a47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PRINCE PEY:\n",
            "Very well! I will be naked to take when.\n",
            "\n",
            "MARIANA:\n",
            "Now sword? and who have friend out of the place\n",
            "Artend I of your vental, and am not pratise\n",
            "He disture friends in Playets to the comprove.\n",
            "\n",
            "LUCIO:\n",
            "By you women, I do put on cominition.\n",
            "And whils woratil orp we moforey any\n",
            "Moris: y maingive, os wacking, waras, t, ouns\n",
            "Olyonesig? fagonad! cin, t, s,\n",
            "I; fo, foce lelinsts!-t tate nope's; war!\n",
            "Trimply titaved ps, ge\n",
            "Fingeivedy, bequbupe, po.\n",
            "Trhokeringe at bous ot: bys; wined iooualy; on\n"
          ]
        }
      ],
      "source": [
        "print(decode(poem_gpt.generate(max_new_tokens=500).cpu().numpy()[0][1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tlSSVccpaSVf"
      },
      "outputs": [],
      "source": [
        "torch.save(obj=poem_gpt.state_dict(),\n",
        "           f=open(\"./weights/poem_gpt_weights.pt\", \"wb\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
